{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔸 https://github.com/BeeGass/HiPPO-Jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -q --upgrade pip\n",
    "# ! pip install -q einops\n",
    "# ! pip install -q tqdm\n",
    "\n",
    "# ! pip install jax \n",
    "# ! pip install -q jaxlib flax jaxtyping typing-extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/usr/local/cuda-12.3/bin:/root/miniconda3/envs/py311/bin:/root/miniconda3/condabin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n",
      "env: LD_LIBRARY_PATH=/usr/local/cuda-12.3/lib64:\n"
     ]
    }
   ],
   "source": [
    "# Capture the current PATH\n",
    "CURR_PATH = !echo $PATH\n",
    "CURR_PATH = CURR_PATH[0]  # Get the string value\n",
    "\n",
    "# Set the new PATH\n",
    "%env PATH=/usr/local/cuda-12.3/bin:{CURR_PATH}\n",
    "\n",
    "# Capture the current LD_LIBRARY_PATH\n",
    "CURR_LD_LIB_PATH = !echo $LD_LIBRARY_PATH\n",
    "CURR_LD_LIB_PATH = CURR_LD_LIB_PATH[0] if CURR_LD_LIB_PATH else \"\"\n",
    "\n",
    "# Set the new LD_LIBRARY_PATH\n",
    "%env LD_LIBRARY_PATH=/usr/local/cuda-12.3/lib64:{CURR_LD_LIB_PATH}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! echo $PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_FLAGS=--xla_dump_to=/tmp/why_is_this_slow.txt\n"
     ]
    }
   ],
   "source": [
    "JIT = True  # Set to False to disable JIT\n",
    "\n",
    "if JIT:\n",
    "    # TODO: set JIT=True and inspect this logfile\n",
    "    # takes > 30min to generate on my macbook\n",
    "    %env XLA_FLAGS=--xla_dump_to=/tmp/why_is_this_slow.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module_path = os.path.abspath(os.path.join(\"../../../../\"))\n",
    "# print(f\"module_path: {module_path}\")\n",
    "# if module_path not in sys.path:\n",
    "#     print(f\"Adding {module_path} to sys.path\")\n",
    "#     sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"False\"\n",
    "\n",
    "# need this so notebook cells reference the same 'cuda runtime'\n",
    "# (something like this)\n",
    "os.environ[\"TF_FORCE_UNIFIED_MEMORY\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "from flax.linen.activation import sigmoid, tanh, relu\n",
    "import numpy as np\n",
    "# import torch\n",
    "from flax.training import train_state  # , orbax_utils\n",
    "import orbax.checkpoint as ocp\n",
    "import optax\n",
    "from jaxtyping import install_import_hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cuda(id=0)]\n"
     ]
    }
   ],
   "source": [
    "print(jax.devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! dpkg -l | grep cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do this one-time if you want CUDA, then restart/rerun notebook\n",
    "INSTALL_JAX_WITH_CUDA = False\n",
    "if INSTALL_JAX_WITH_CUDA:\n",
    "    # instruction from https://github.com/google/jax?tab=readme-ov-file#installation:\n",
    "    ! pip install -U \"jax[cuda12_pip]\" -f  \\\n",
    "        https://storage.googleapis.com/jax-releases/jax_cuda_releases.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[cuda(id=0)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Device: gpu\n"
     ]
    }
   ],
   "source": [
    "print(f\"The Device: {jax.lib.xla_bridge.get_backend().platform}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"MPS available: {torch.backends.mps.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_printoptions(linewidth=150)\n",
    "np.set_printoptions(linewidth=150)\n",
    "jnp.set_printoptions(linewidth=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1701\n",
    "key = jax.random.PRNGKey(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_copies = 20\n",
    "subkeys = jax.random.split(key, num=num_copies)\n",
    "key = subkeys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hippo_live import HiPPOCell, HiPPOLTI\n",
    "\n",
    "from cells_live import LSTMCell, BatchedGatedHiPPOCell, CharRNN\n",
    "\n",
    "from trans import initializer, legt, legs, lmu, lagt, fru, fout, foud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters For Generating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of measure (tot. area under curve of time-weighting)\n",
    "T = 1\n",
    "\n",
    "# freq = 10\n",
    "\n",
    "# if input is 28x28 (MNIST) and we want to look at each pixel, we want 1 / 28x28\n",
    "# if we want 2x2 pixels we'll want 1/4 / 28x28\n",
    "# etc.\n",
    "STEP = 4 / (28 * 28)   # 1e-3\n",
    "\n",
    "# length of sequence\n",
    "L = int(T / STEP)  # e.g. 1 * 28x28 in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "data_size = L\n",
    "input_size = 1\n",
    "_block_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sequences = 100\n",
    "epochs = 10\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters For HiPPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset (TinyShakespeare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../datasets/shakespeare.txt\", \"r\", encoding=\"latin-1\") as f:\n",
    "    text = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get all the unique characters that occur in this text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all the unique characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocab size: 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"all the unique characters:\", \"\".join(chars))\n",
    "print(f\"vocab size: {vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a mapping from characters to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(s):\n",
    "    int_list = []\n",
    "    for c in s:\n",
    "        if c not in stoi:\n",
    "            raise ValueError(f\"character {c} not in vocabulary\")\n",
    "        else:\n",
    "            int_list.append(\n",
    "                stoi[c]\n",
    "            )  # encoder: take a string, output a list of integers\n",
    "    return int_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(l):\n",
    "    str_list = []\n",
    "    for i in l:\n",
    "        if i not in itos:\n",
    "            raise ValueError(f\"integer {i} not in the vocabulary\")\n",
    "        else:\n",
    "            str_list.append(\n",
    "                itos[i]\n",
    "            )  # decoder: take a list of integers, output a list of characters\n",
    "    return \"\".join(str_list)  # take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 46, 43, 1, 44, 53, 62, 1, 48, 59, 51, 54, 57, 1, 53, 60, 43, 56, 1, 58, 46, 43, 1, 50, 39, 64, 63, 1, 42, 53, 45, 8]\n",
      "The fox jumps over the lazy dog.\n"
     ]
    }
   ],
   "source": [
    "txt = \"The fox jumps over the lazy dog.\"\n",
    "print(encode(txt))\n",
    "print(decode(encode(txt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1115394,)\n",
      "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43 39\n",
      " 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 37\n",
      " 53 59]\n"
     ]
    }
   ],
   "source": [
    "data = jnp.array(encode(text))\n",
    "print(data.shape)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now split up the data into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 1,003,854 tokens\n",
      "train has shape (1003854,)\n",
      "\n",
      "val has 111,540 tokens\n",
      "val has shape (111540,)\n"
     ]
    }
   ],
   "source": [
    "n = int(0.9 * len(data))  # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(f\"train has {len(train_data):,} tokens\")\n",
    "print(f\"train has shape {train_data.shape}\\n\")\n",
    "print(f\"val has {len(val_data):,} tokens\")\n",
    "print(f\"val has shape {val_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([18, 47, 56, 57, 58,  1, 15, 47, 58], dtype=int32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[: block_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is [18] the target: 47\n",
      "when input is [18 47] the target: 56\n",
      "when input is [18 47 56] the target: 57\n",
      "when input is [18 47 56 57] the target: 58\n",
      "when input is [18 47 56 57 58] the target: 1\n",
      "when input is [18 47 56 57 58  1] the target: 15\n",
      "when input is [18 47 56 57 58  1 15] the target: 47\n",
      "when input is [18 47 56 57 58  1 15 47] the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1 : block_size + 1]\n",
    "for t in range(block_size):\n",
    "    context = x[: t + 1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(data, block_size=8):\n",
    "    # Trim the data to ensure that it is divisible by block_size for proper reshaping\n",
    "    num_complete_blocks = (data.size - 1) // block_size\n",
    "    trimmed_data_size = num_complete_blocks * block_size + 1\n",
    "    data = data[:trimmed_data_size]\n",
    "\n",
    "    # Prepare the input data 'x'\n",
    "    x = data[:-1].reshape(num_complete_blocks, block_size)\n",
    "\n",
    "    # Prepare the target data 'y'\n",
    "    # The target for each sequence in 'x' is the sequence shifted by one token\n",
    "    y = data[1:].reshape(num_complete_blocks, block_size)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_target_relation(x_ds, y_ds, bs, block_size):\n",
    "    data_size = len(x_ds)\n",
    "    target_size = len(y_ds)\n",
    "    steps_per_epoch = data_size // bs\n",
    "\n",
    "    perms = jax.random.permutation(subkeys[0], data_size)\n",
    "    perms = perms[: steps_per_epoch * bs]  # skip incomplete batch\n",
    "    perms = perms.reshape((steps_per_epoch, bs))\n",
    "\n",
    "    for i, perm in enumerate(perms):\n",
    "        print(f\"inputs: {i}\")\n",
    "        print(x_ds[perm, ...].shape)\n",
    "        print(f\"{x_ds[perm, ...]}\\n\")\n",
    "        print(f\"targets: {i}\")\n",
    "        print(y_ds[perm, ...].shape)\n",
    "        print(y_ds[perm, ...])\n",
    "        print(\"----\")\n",
    "        for b in range(bs):  # batch dimension\n",
    "            for t in range(block_size):  # time dimension\n",
    "                context = x_ds[perm, ...][b, : t + 1]\n",
    "                target = y_ds[perm, ...][b, t]\n",
    "                print(f\"when input is {context.tolist()} the target: {target}\")\n",
    "        print(\"--------\")\n",
    "        if i == 2:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_ds shape: (125481, 8)\n",
      "y_ds shape: (125481, 8)\n"
     ]
    }
   ],
   "source": [
    "x_ds, y_ds = batch_data(data=train_data, block_size=block_size)\n",
    "print(f\"x_ds shape: {x_ds.shape}\")\n",
    "print(f\"y_ds shape: {y_ds.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_target_relation(x_ds, y_ds, bs=4, block_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HiPPO](resources/hippo.png)\n",
    "\n",
    "page 55 of \"Efficient HiPPO With Flax: Tackling Long Term Dependencies in Deep Learning\" By Bryan Gass\n",
    "\n",
    "https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the whole shebang\n",
    "# in our case, we have 2 layers\n",
    "# each layer is an 'rnn' cell (in our terminology) which contains \n",
    "#  a hippo cell and an LSTM cell\n",
    "def initialize_rnn(cell_list, method: str, ALPHA: float = 2.0):\n",
    "    assert method in [\"legs\", \"legt\", \"lmu\", \"lagt\", \"fru\", \"fout\", \"foud\"]\n",
    "    INIT_FN = {\n",
    "        \"legs\": legs,\n",
    "        \"legt\": legt,\n",
    "        \"lmu\": lmu,\n",
    "        \"lagt\": lagt,\n",
    "        \"fru\": fru,\n",
    "        \"fout\": fout,\n",
    "        \"foud\": foud,\n",
    "        # \"chebt\": chebt,\n",
    "    }[method]\n",
    "\n",
    "    # within an RNN cell we have 2 cells:\n",
    "    # - Hippo\n",
    "    # - Tau\n",
    "    cell_args = [\n",
    "        {\n",
    "            \"hippo_cell\": HiPPOLTI,  # HiPPOLTICell,\n",
    "            \"hippo_args\": {\n",
    "                \"step_size\": STEP,\n",
    "                \"basis_size\": T,\n",
    "                \"alpha\": ALPHA,\n",
    "                # \"recon\": False,\n",
    "                \"A_init\": INIT_FN,\n",
    "                \"B_init\": INIT_FN,\n",
    "            },\n",
    "            # tau represents arb. RNN cell\n",
    "            # so these args are common to different RNN schemes; LSTM GRU HiPPO etc.\n",
    "            \"tau_args\": {\n",
    "                \"features\": N,  # hidden dimension\n",
    "                \"bias\": True,  # store a bias value? (we init to 0.0)\n",
    "                \"gate_fn\": sigmoid,\n",
    "                \"activation_fn\": tanh,\n",
    "                \"dtype\": jnp.float32,\n",
    "            },\n",
    "            # \"mlp_args\": {\n",
    "            #     \"features\": [1],\n",
    "            #     \"activation_fn\": relu,\n",
    "            #     \"bias\": False,\n",
    "            #     \"dtype\": jnp.float32,\n",
    "            # },\n",
    "            \"_tau\": LSTMCell,\n",
    "            \"bias\": True,\n",
    "            \"dtype\": jnp.float32,\n",
    "        }\n",
    "        for i in range(len(cell_list))\n",
    "    ]\n",
    "\n",
    "    rnn = CharRNN(\n",
    "        vocab_size=vocab_size,  # we use this in the final classification layer\n",
    "        hidden_size=N,\n",
    "        rnn_cells=cell_list,\n",
    "        cell_args=cell_args,\n",
    "    )\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "qq = initialize_rnn(\n",
    "        cell_list=[BatchedGatedHiPPOCell, BatchedGatedHiPPOCell],\n",
    "        method=\"legs\",\n",
    "        ALPHA=2.0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharRNN(\n",
       "    # attributes\n",
       "    vocab_size = 65\n",
       "    hidden_size = 16\n",
       "    rnn_cells = [<class 'cells_live.BatchedGatedHiPPOCell'>, <class 'cells_live.BatchedGatedHiPPOCell'>]\n",
       "    cell_args = [{'hippo_cell': <class 'hippo_live.HiPPOLTI'>, 'hippo_args': {'step_size': 0.00510204081632653, 'basis_size': 1, 'alpha': 2.0, 'A_init': <function legs at 0x7f2a1c1993a0>, 'B_init': <function legs at 0x7f2a1c1993a0>}, 'tau_args': {'features': 16, 'bias': True, 'gate_fn': <PjitFunction of <function sigmoid at 0x7f2a668c1440>>, 'activation_fn': <PjitFunction of <function jax.numpy.tanh at 0x7f2a66ee1120>>, 'dtype': <class 'jax.numpy.float32'>}, '_tau': <class 'cells_live.LSTMCell'>, 'bias': True, 'dtype': <class 'jax.numpy.float32'>}, {'hippo_cell': <class 'hippo_live.HiPPOLTI'>, 'hippo_args': {'step_size': 0.00510204081632653, 'basis_size': 1, 'alpha': 2.0, 'A_init': <function legs at 0x7f2a1c1993a0>, 'B_init': <function legs at 0x7f2a1c1993a0>}, 'tau_args': {'features': 16, 'bias': True, 'gate_fn': <PjitFunction of <function sigmoid at 0x7f2a668c1440>>, 'activation_fn': <PjitFunction of <function jax.numpy.tanh at 0x7f2a66ee1120>>, 'dtype': <class 'jax.numpy.float32'>}, '_tau': <class 'cells_live.LSTMCell'>, 'bias': True, 'dtype': <class 'jax.numpy.float32'>}]\n",
       "    dtype = float32\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_shaping():\n",
    "    # Define the model\n",
    "    cell_list = [BatchedGatedHiPPOCell, BatchedGatedHiPPOCell]\n",
    "    \n",
    "    # within an RNN cell we have 2 cells:\n",
    "    # - Hippo\n",
    "    # - Tau\n",
    "    cell_args = [\n",
    "        {\n",
    "            \"hippo_cell\": HiPPOLTI,  # HiPPOLTICell\n",
    "            \"hippo_args\": {\n",
    "                \"step_size\": STEP, # the step size associated with the time weighting\n",
    "                \"basis_size\": T, # the size of the basis functions\n",
    "                \"alpha\": 2.0, # choose zero-order hold\n",
    "                # \"recon\": False, # we dont need to reconstruct the underlying signal\n",
    "                \"A_init\": legs, # initialize the A matrix with HiPPO-LegS\n",
    "                \"B_init\": legs, # initialize the B matrix with HiPPO-LegS\n",
    "            },\n",
    "            # tau represents arb. RNN cell\n",
    "            # so these args are common to different RNN schemes; LSTM GRU HiPPO etc.\n",
    "            \"tau_args\": {\n",
    "                \"features\": N,  # hidden dimension\n",
    "                \"bias\": True,  # store a bias value? (we init to 0.0)\n",
    "                \"gate_fn\": sigmoid, # gating mechanism for the LSTM (can be gating mechanism for GRU etc.)\n",
    "                \"activation_fn\": tanh, # activation function for the LSTM (can be activation function for GRU etc.)\n",
    "                \"dtype\": jnp.float32, # data type\n",
    "            },\n",
    "            \"_tau\": LSTMCell, # the arb. RNN cell is an LSTM cell\n",
    "            \"bias\": True, # store a bias value? (we init to 0.0)\n",
    "            \"dtype\": jnp.float32, # data type\n",
    "        }\n",
    "        for i in range(len(cell_list)) # we have 2 layers in this case\n",
    "    ]\n",
    "\n",
    "    model = CharRNN(\n",
    "        vocab_size=vocab_size,  # we use this in the final classification layer\n",
    "        hidden_size=N, # hidden size\n",
    "        rnn_cells=cell_list, # the RNN cells \n",
    "        cell_args=cell_args, # the args for the RNN cells\n",
    "    )\n",
    "    \n",
    "    tabulate_fn = nn.tabulate(\n",
    "    CharRNN(\n",
    "        vocab_size=vocab_size,  # we use this in the final classification layer\n",
    "        hidden_size=N, # hidden size\n",
    "        rnn_cells=cell_list, # the RNN cells \n",
    "        cell_args=cell_args, # the args for the RNN cells\n",
    "    ), jax.random.key(0), compute_flops=True, compute_vjp_flops=True)\n",
    "\n",
    "    # Define the input\n",
    "    input_data = jnp.ones((batch_size, block_size), dtype=jnp.int32)\n",
    "    print(f\"input data shape: {input_data.shape}\")\n",
    "\n",
    "    # Initialize the model carries\n",
    "    carries = model.initialize_carries(\n",
    "        rng=subkeys[7], batch_size=batch_size, hidden_sizes=[N, N]\n",
    "    )\n",
    "    print(f\"hidden shape: {carries}\")\n",
    "    print(f\"hidden shape: {carries[-1][0].shape}\")\n",
    "\n",
    "    # Initialize the model parameters\n",
    "    params = model.init(\n",
    "        subkeys[1],\n",
    "        x=input_data,\n",
    "        carry=carries,\n",
    "        targets=None,\n",
    "    )\n",
    "\n",
    "    # Initialize the model carries\n",
    "    carries = model.initialize_carries(\n",
    "        rng=subkeys[8], batch_size=batch_size, hidden_sizes=[N, N]\n",
    "    )\n",
    "    print(f\"hidden shape: {carries[-1][0].shape}\")\n",
    "    \n",
    "    print(tabulate_fn(x=input_data, carry=carries, targets=None))\n",
    "\n",
    "    # Apply the model to the input\n",
    "    output, new_carries = model.apply(\n",
    "        params, x=input_data, carry=carries, targets=None\n",
    "    )\n",
    "\n",
    "    # Check the output shape\n",
    "    print(f\"output shape: {output.shape}\")\n",
    "    assert output.shape == (batch_size, block_size, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline: 448,737\n",
    "# reduce batch size by 2x: -> 225,441\n",
    "# reduce STEP size by 4x (L goes down from 784 to 196) -> 448,737"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input data shape: (64, 8)\n",
      "hidden shape: [(Array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), Array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)), (Array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), Array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32))]\n",
      "hidden shape: (64, 16)\n",
      "hidden shape: (64, 16)\n",
      "\n",
      "\u001b[3m                                CharRNN Summary                                 \u001b[0m\n",
      "┏━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mpath     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmodule  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1minputs   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutputs \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mflops  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mvjp_flops\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mparams  \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━┩\n",
      "│           │ CharRNN  │ carry:    │ -        │ 8172033 │ 26752016  │          │\n",
      "│           │          │ - -       │ \u001b[2mfloat32\u001b[0m… │         │           │          │\n",
      "│           │          │ \u001b[2mfloat32\u001b[0m[… │ - - -    │         │           │          │\n",
      "│           │          │   -       │ \u001b[2mfloat32\u001b[0m… │         │           │          │\n",
      "│           │          │ \u001b[2mfloat32\u001b[0m[… │     -    │         │           │          │\n",
      "│           │          │ - -       │ \u001b[2mfloat32\u001b[0m… │         │           │          │\n",
      "│           │          │ \u001b[2mfloat32\u001b[0m[… │   - -    │         │           │          │\n",
      "│           │          │   -       │ \u001b[2mfloat32\u001b[0m… │         │           │          │\n",
      "│           │          │ \u001b[2mfloat32\u001b[0m[… │     -    │         │           │          │\n",
      "│           │          │ targets:  │ \u001b[2mfloat32\u001b[0m… │         │           │          │\n",
      "│           │          │ None      │          │         │           │          │\n",
      "│           │          │ x:        │          │         │           │          │\n",
      "│           │          │ \u001b[2mint32\u001b[0m[64… │          │         │           │          │\n",
      "├───────────┼──────────┼───────────┼──────────┼─────────┼───────────┼──────────┤\n",
      "│ embedding │ Embed    │ \u001b[2mint32\u001b[0m[64… │ \u001b[2mfloat32\u001b[0m… │ 11265   │ 20497     │ embeddi… │\n",
      "│           │          │           │          │         │           │ \u001b[2mfloat32\u001b[0m… │\n",
      "│           │          │           │          │         │           │          │\n",
      "│           │          │           │          │         │           │ \u001b[1m1,040 \u001b[0m   │\n",
      "│           │          │           │          │         │           │ \u001b[1;2m(4.2 KB)\u001b[0m │\n",
      "├───────────┼──────────┼───────────┼──────────┼─────────┼───────────┼──────────┤\n",
      "│ cells_0   │ Batched… │ - -       │ -        │ 441408  │ 1446912   │          │\n",
      "│           │          │ \u001b[2mfloat32\u001b[0m[… │ \u001b[2mfloat32\u001b[0m… │         │           │          │\n",
      "│           │          │   -       │ -        │         │           │          │\n",
      "│           │          │ \u001b[2mfloat32\u001b[0m[… │ \u001b[2mfloat32\u001b[0m… │         │           │          │\n",
      "│           │          │ -         │          │         │           │          │\n",
      "│           │          │ \u001b[2mfloat32\u001b[0m[… │          │         │           │          │\n",
      "├───────────┼──────────┼───────────┼──────────┼─────────┼───────────┼──────────┤\n",
      "│ cells_0/… │ GateHiP… │ - -       │ -        │ 6897    │ 22608     │          │\n",
      "│           │          │ \u001b[2mfloat32\u001b[0m[… │ \u001b[2mfloat32\u001b[0m… │         │           │          │\n",
      "│           │          │   -       │ -        │         │           │          │\n",
      "│           │          │ \u001b[2mfloat32\u001b[0m[… │ \u001b[2mfloat32\u001b[0m… │         │           │          │\n",
      "│           │          │ -         │          │         │           │          │\n",
      "│           │          │ \u001b[2mfloat32\u001b[0m[… │          │         │           │          │\n",
      "├───────────┼──────────┼───────────┼──────────┼─────────┼───────────┼──────────┤\n",
      "│ cells_0/… │ LSTMCell │ - -       │ -        │ 6544    │ 22608     │          │\n",
      "│           │          │ \u001b[2mfloat32\u001b[0m[… │ \u001b[2mfloat32\u001b[0m… │         │           │          │\n",
      "│           │          │   -       │ -        │         │           │          │\n",
      "│           │          │ \u001b[2mfloat32\u001b[0m[… │ \u001b[2mfloat32\u001b[0m… │         │           │          │\n",
      "│           │          │ -         │          │         │           │          │\n",
      "│           │          │ \u001b[2mfloat32\u001b[0m[… │          │         │           │          │\n",
      "├───────────┼──────────┼───────────┼──────────┼─────────┼───────────┼──────────┤\n",
      "│ cells_0/… │ Dense    │ \u001b[2mfloat32\u001b[0m[… │ \u001b[2mfloat32\u001b[0m… │ 4160    │ 14464     │ bias:    │\n",
      "│           │          │           │          │         │           │ \u001b[2mfloat32\u001b[0m… │\n",
      "│           │          │           │          │         │           │ kernel:  │\n",
      "│           │          │           │          │         │           │ \u001b[2mfloat32\u001b[0m… │\n",
      "│           │          │           │          │         │           │          │\n",
      "│           │          │           │          │         │           │ \u001b[1m135,168 \u001b[0m │\n",
      "│           │          │           │          │         │           │ \u001b[1;2m(540.7 \u001b[0m  │\n",
      "│           │          │           │          │         │           │ \u001b[1;2mKB)\u001b[0m      │\n",
      "├───────────┼──────────┼───────────┼──────────┼─────────┼───────────┼──────────┤\n",
      "│ cells_0/… │ Dense    │ \u001b[2mfloat32\u001b[0m[… │ \u001b[2mfloat32\u001b[0m… │ 2112    │ 7296      │ bias:    │\n",
      "│           │          │           │          │         │           │ \u001b[2mfloat32\u001b[0m… │\n",
      "│           │          │           │          │         │           │ kernel:  │\n",
      "│           │          │           │          │         │           │ \u001b[2mfloat32\u001b[0m… │\n",
      "│           │          │           │          │         │           │          │\n",
      "│           │          │           │          │         │           │ \u001b[1m69,632 \u001b[0m  │\n",
      "│           │          │           │          │         │           │ \u001b[1;2m(278.5 \u001b[0m  │\n",
      "│           │          │           │          │         │           │ \u001b[1;2mKB)\u001b[0m      │\n",
      "├───────────┼──────────┼───────────┼──────────┼─────────┼───────────┼──────────┤\n",
      "│ cells_0/… │ Dense    │ \u001b[2mfloat32\u001b[0m[… │ \u001b[2mfloat32\u001b[0m… │ 33      │ 114       │ bias:    │\n",
      "│           │          │           │          │         │           │ \u001b[2mfloat32\u001b[0m… │\n",
      "│           │          │           │          │         │           │ kernel:  │\n",
      "│           │          │           │          │         │           │ \u001b[2mfloat32\u001b[0m… │\n",
      "│           │          │           │          │         │           │          │\n",
      "│           │          │           │          │         │           │ \u001b[1m1,088 \u001b[0m   │\n",
      "│           │          │           │          │         │           │ \u001b[1;2m(4.4 KB)\u001b[0m │\n",
      "├───────────┼──────────┼───────────┼──────────┼─────────┼───────────┼──────────┤\n",
      "│ cells_0/… │ HiPPOLTI │ -         │ -        │ 320     │ 736       │ A_d:     │\n",
      "│           │          │ \u001b[2mfloat32\u001b[0m[… │ \u001b[2mfloat32\u001b[0m… │         │           │ \u001b[2mfloat32\u001b[0m… │\n",
      "│           │          │ -         │ -        │         │           │ B_d:     │\n",
      "│           │          │ \u001b[2mfloat32\u001b[0m[… │ \u001b[2mfloat32\u001b[0m… │         │           │ \u001b[2mfloat32\u001b[0m… │\n",
      "│           │          │           │          │         │           │          │\n",
      "│           │          │           │          │         │           │ \u001b[1m17,408 \u001b[0m  │\n",
      "│           │          │           │          │         │           │ \u001b[1;2m(69.6 \u001b[0m   │\n",
      "│           │          │           │          │         │           │ \u001b[1;2mKB)\u001b[0m      │\n",
      "├───────────┼──────────┼───────────┼──────────┼─────────┼───────────┼──────────┤\n",
      "│ cells_1   │ Batched… │ - -       │ -        │ 441408  │ 1446912   │          │\n",
      "│           │          │ \u001b[2mfloat32\u001b[0m[… │ \u001b[2mfloat32\u001b[0m… │         │           │          │\n",
      "│           │          │   -       │ -        │         │           │          │\n",
      "│           │          │ \u001b[2mfloat32\u001b[0m[… │ \u001b[2mfloat32\u001b[0m… │         │           │          │\n",
      "│           │          │ -         │          │         │           │          │\n",
      "│           │          │ \u001b[2mfloat32\u001b[0m[… │          │         │           │          │\n",
      "├───────────┼──────────┼───────────┼──────────┼─────────┼───────────┼──────────┤\n",
      "│ cells_1/… │ GateHiP… │ - -       │ -        │ 6897    │ 22608     │          │\n",
      "│           │          │ \u001b[2mfloat32\u001b[0m[… │ \u001b[2mfloat32\u001b[0m… │         │           │          │\n",
      "│           │          │   -       │ -        │         │           │          │\n",
      "│           │          │ \u001b[2mfloat32\u001b[0m[… │ \u001b[2mfloat32\u001b[0m… │         │           │          │\n",
      "│           │          │ -         │          │         │           │          │\n",
      "│           │          │ \u001b[2mfloat32\u001b[0m[… │          │         │           │          │\n",
      "├───────────┼──────────┼───────────┼──────────┼─────────┼───────────┼──────────┤\n",
      "│ cells_1/… │ LSTMCell │ - -       │ -        │ 6544    │ 22608     │          │\n",
      "│           │          │ \u001b[2mfloat32\u001b[0m[… │ \u001b[2mfloat32\u001b[0m… │         │           │          │\n",
      "│           │          │   -       │ -        │         │           │          │\n",
      "│           │          │ \u001b[2mfloat32\u001b[0m[… │ \u001b[2mfloat32\u001b[0m… │         │           │          │\n",
      "│           │          │ -         │          │         │           │          │\n",
      "│           │          │ \u001b[2mfloat32\u001b[0m[… │          │         │           │          │\n",
      "├───────────┼──────────┼───────────┼──────────┼─────────┼───────────┼──────────┤\n",
      "│ cells_1/… │ Dense    │ \u001b[2mfloat32\u001b[0m[… │ \u001b[2mfloat32\u001b[0m… │ 4160    │ 14464     │ bias:    │\n",
      "│           │          │           │          │         │           │ \u001b[2mfloat32\u001b[0m… │\n",
      "│           │          │           │          │         │           │ kernel:  │\n",
      "│           │          │           │          │         │           │ \u001b[2mfloat32\u001b[0m… │\n",
      "│           │          │           │          │         │           │          │\n",
      "│           │          │           │          │         │           │ \u001b[1m135,168 \u001b[0m │\n",
      "│           │          │           │          │         │           │ \u001b[1;2m(540.7 \u001b[0m  │\n",
      "│           │          │           │          │         │           │ \u001b[1;2mKB)\u001b[0m      │\n",
      "├───────────┼──────────┼───────────┼──────────┼─────────┼───────────┼──────────┤\n",
      "│ cells_1/… │ Dense    │ \u001b[2mfloat32\u001b[0m[… │ \u001b[2mfloat32\u001b[0m… │ 2112    │ 7296      │ bias:    │\n",
      "│           │          │           │          │         │           │ \u001b[2mfloat32\u001b[0m… │\n",
      "│           │          │           │          │         │           │ kernel:  │\n",
      "│           │          │           │          │         │           │ \u001b[2mfloat32\u001b[0m… │\n",
      "│           │          │           │          │         │           │          │\n",
      "│           │          │           │          │         │           │ \u001b[1m69,632 \u001b[0m  │\n",
      "│           │          │           │          │         │           │ \u001b[1;2m(278.5 \u001b[0m  │\n",
      "│           │          │           │          │         │           │ \u001b[1;2mKB)\u001b[0m      │\n",
      "├───────────┼──────────┼───────────┼──────────┼─────────┼───────────┼──────────┤\n",
      "│ cells_1/… │ Dense    │ \u001b[2mfloat32\u001b[0m[… │ \u001b[2mfloat32\u001b[0m… │ 33      │ 114       │ bias:    │\n",
      "│           │          │           │          │         │           │ \u001b[2mfloat32\u001b[0m… │\n",
      "│           │          │           │          │         │           │ kernel:  │\n",
      "│           │          │           │          │         │           │ \u001b[2mfloat32\u001b[0m… │\n",
      "│           │          │           │          │         │           │          │\n",
      "│           │          │           │          │         │           │ \u001b[1m1,088 \u001b[0m   │\n",
      "│           │          │           │          │         │           │ \u001b[1;2m(4.4 KB)\u001b[0m │\n",
      "├───────────┼──────────┼───────────┼──────────┼─────────┼───────────┼──────────┤\n",
      "│ cells_1/… │ HiPPOLTI │ -         │ -        │ 320     │ 736       │ A_d:     │\n",
      "│           │          │ \u001b[2mfloat32\u001b[0m[… │ \u001b[2mfloat32\u001b[0m… │         │           │ \u001b[2mfloat32\u001b[0m… │\n",
      "│           │          │ -         │ -        │         │           │ B_d:     │\n",
      "│           │          │ \u001b[2mfloat32\u001b[0m[… │ \u001b[2mfloat32\u001b[0m… │         │           │ \u001b[2mfloat32\u001b[0m… │\n",
      "│           │          │           │          │         │           │          │\n",
      "│           │          │           │          │         │           │ \u001b[1m17,408 \u001b[0m  │\n",
      "│           │          │           │          │         │           │ \u001b[1;2m(69.6 \u001b[0m   │\n",
      "│           │          │           │          │         │           │ \u001b[1;2mKB)\u001b[0m      │\n",
      "├───────────┼──────────┼───────────┼──────────┼─────────┼───────────┼──────────┤\n",
      "│ output_p… │ Dense    │ \u001b[2mfloat32\u001b[0m[… │ \u001b[2mfloat32\u001b[0m… │ 137280  │ 408720    │ bias:    │\n",
      "│           │          │           │          │         │           │ \u001b[2mfloat32\u001b[0m… │\n",
      "│           │          │           │          │         │           │ kernel:  │\n",
      "│           │          │           │          │         │           │ \u001b[2mfloat32\u001b[0m… │\n",
      "│           │          │           │          │         │           │          │\n",
      "│           │          │           │          │         │           │ \u001b[1m1,105 \u001b[0m   │\n",
      "│           │          │           │          │         │           │ \u001b[1;2m(4.4 KB)\u001b[0m │\n",
      "├───────────┼──────────┼───────────┼──────────┼─────────┼───────────┼──────────┤\n",
      "│\u001b[1m \u001b[0m\u001b[1m         \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m        \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m         \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m        \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m       \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m    Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m448,737 \u001b[0m\u001b[1m \u001b[0m│\n",
      "│\u001b[1m           \u001b[0m│\u001b[1m          \u001b[0m│\u001b[1m           \u001b[0m│\u001b[1m          \u001b[0m│\u001b[1m         \u001b[0m│\u001b[1m           \u001b[0m│\u001b[1m \u001b[0m\u001b[1;2m(1.8 MB)\u001b[0m\u001b[1m \u001b[0m│\n",
      "└───────────┴──────────┴───────────┴──────────┴─────────┴───────────┴──────────┘\n",
      "\u001b[1m                                                                                \u001b[0m\n",
      "\u001b[1m                       Total Parameters: 448,737 \u001b[0m\u001b[1;2m(1.8 MB)\u001b[0m\u001b[1m                       \u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "UnexpectedTracerError",
     "evalue": "Encountered an unexpected tracer. A function transformed by JAX had a side effect, allowing for a reference to an intermediate value with type float32[64,16] wrapped in a DynamicJaxprTracer to escape the scope of the transformation.\nJAX transformations require that functions explicitly return their outputs, and disallow saving intermediate values to global state.\nThe function being traced when the value leaked was _get_variables at /root/miniconda3/envs/py311/lib/python3.11/site-packages/flax/linen/summary.py:432 traced for eval_shape.\n------------------------------\nThe leaked intermediate value was created on line /root/ssm/hippo/cells_live.py:105:14 (GateHiPPOCell.__call__). \n------------------------------\nWhen the value was created, the final 5 stack frames (most recent last) excluding JAX-internal frames were:\n------------------------------\n/tmp/ipykernel_9696/806859196.py:1 (<module>)\n/tmp/ipykernel_9696/230873890.py:75:10 (test_shaping)\n/root/ssm/hippo/cells_live.py:189:33 (CharRNN.__call__)\n/root/ssm/hippo/cells_live.py:145:15 (BatchedGatedHiPPOCell.__call__)\n/root/ssm/hippo/cells_live.py:105:14 (GateHiPPOCell.__call__)\n------------------------------\n\nTo catch the leak earlier, try setting the environment variable JAX_CHECK_TRACER_LEAKS or using the `jax.checking_leaks` context manager.\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.UnexpectedTracerError",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedTracerError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_shaping\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[48], line 78\u001b[0m, in \u001b[0;36mtest_shaping\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(tabulate_fn(x\u001b[38;5;241m=\u001b[39minput_data, carry\u001b[38;5;241m=\u001b[39mcarries, targets\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Apply the model to the input\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m output, new_carries \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcarry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcarries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m     80\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Check the output shape\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "    \u001b[0;31m[... skipping hidden 6 frame]\u001b[0m\n",
      "File \u001b[0;32m~/ssm/hippo/cells_live.py:189\u001b[0m, in \u001b[0;36mCharRNN.__call__\u001b[0;34m(self, carry, x, targets)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, cell \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcells):\n\u001b[1;32m    188\u001b[0m     cell_input \u001b[38;5;241m=\u001b[39m embedded \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m new_carries[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 189\u001b[0m     new_carries[i] \u001b[38;5;241m=\u001b[39m \u001b[43mcell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_carries\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m logit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_proj(new_carries[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    192\u001b[0m logits_sequence\u001b[38;5;241m.\u001b[39mappend(logit)\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/ssm/hippo/cells_live.py:145\u001b[0m, in \u001b[0;36mBatchedGatedHiPPOCell.__call__\u001b[0;34m(self, carry, x)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, carry, x):\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcarry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "File \u001b[0;32m~/ssm/hippo/cells_live.py:88\u001b[0m, in \u001b[0;36mGateHiPPOCell.__call__\u001b[0;34m(self, carry, x)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, carry, x):\n\u001b[1;32m     87\u001b[0m     h_t, c_t_1 \u001b[38;5;241m=\u001b[39m carry\n\u001b[0;32m---> 88\u001b[0m     tau_x \u001b[38;5;241m=\u001b[39m \u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc_t_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;66;03m# get the hidden state from the RNN of choice\u001b[39;00m\n\u001b[1;32m     91\u001b[0m     h_t, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtau(carry, tau_x)\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/jax/_src/interpreters/partial_eval.py:1698\u001b[0m, in \u001b[0;36mDynamicJaxprTracer._assert_live\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1696\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_assert_live\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1697\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trace\u001b[38;5;241m.\u001b[39mmain\u001b[38;5;241m.\u001b[39mjaxpr_stack:  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m-> 1698\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39mescaped_tracer_error(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mUnexpectedTracerError\u001b[0m: Encountered an unexpected tracer. A function transformed by JAX had a side effect, allowing for a reference to an intermediate value with type float32[64,16] wrapped in a DynamicJaxprTracer to escape the scope of the transformation.\nJAX transformations require that functions explicitly return their outputs, and disallow saving intermediate values to global state.\nThe function being traced when the value leaked was _get_variables at /root/miniconda3/envs/py311/lib/python3.11/site-packages/flax/linen/summary.py:432 traced for eval_shape.\n------------------------------\nThe leaked intermediate value was created on line /root/ssm/hippo/cells_live.py:105:14 (GateHiPPOCell.__call__). \n------------------------------\nWhen the value was created, the final 5 stack frames (most recent last) excluding JAX-internal frames were:\n------------------------------\n/tmp/ipykernel_9696/806859196.py:1 (<module>)\n/tmp/ipykernel_9696/230873890.py:75:10 (test_shaping)\n/root/ssm/hippo/cells_live.py:189:33 (CharRNN.__call__)\n/root/ssm/hippo/cells_live.py:145:15 (BatchedGatedHiPPOCell.__call__)\n/root/ssm/hippo/cells_live.py:105:14 (GateHiPPOCell.__call__)\n------------------------------\n\nTo catch the leak earlier, try setting the environment variable JAX_CHECK_TRACER_LEAKS or using the `jax.checking_leaks` context manager.\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.UnexpectedTracerError"
     ]
    }
   ],
   "source": [
    "test_shaping()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "The cells below will be dedicated towards training a character-level RNN on a given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model(state, carries, input, target, vocab_size, train=True):\n",
    "    def loss_fn(params):\n",
    "        # Use targets for teacher forcing if training\n",
    "        targets = target if train else None\n",
    "        logits, new_carries = state.apply_fn(\n",
    "            params, x=input, carry=carries, targets=targets\n",
    "        )\n",
    "\n",
    "        # One-hot encode the target with the correct vocabulary size\n",
    "        one_hot = jax.nn.one_hot(target, vocab_size)\n",
    "        loss = jnp.mean(optax.softmax_cross_entropy(logits=logits, labels=one_hot))\n",
    "        return loss, (logits, new_carries)\n",
    "\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, (logits, new_carries)), grads = grad_fn(state.params)\n",
    "\n",
    "    # Ensure accuracy calculation matches the target structure\n",
    "    accuracy = jnp.mean(jnp.argmax(logits, -1) == target)\n",
    "    return grads, loss, accuracy, jnp.argmax(logits, axis=-1)\n",
    "\n",
    "if JIT:\n",
    "    apply_model = partial(jax.jit, static_argnames=[\"vocab_size\"])(apply_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model(state, grad):\n",
    "    return state.apply_gradients(grads=grad)\n",
    "\n",
    "if JIT:\n",
    "    update_model = jax.jit(update_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(rng, model, train_ds, state, vocab_size, batch_size=64):\n",
    "    \"\"\"Train for a single epoch.\"\"\"\n",
    "    train_x, train_y = train_ds\n",
    "    rng, permute_rng = jax.random.split(rng, 2)\n",
    "    data_size = len(train_x)\n",
    "    steps_per_epoch = data_size // batch_size\n",
    "    perms = jax.random.permutation(subkeys[0], data_size)\n",
    "    perms = perms[: steps_per_epoch * batch_size]  # skip incomplete batch\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "    epoch_loss = []\n",
    "    epoch_accuracy = []\n",
    "    for perm in tqdm(perms):\n",
    "        input = train_x[perm, ...]\n",
    "        target = train_y[perm, ...]\n",
    "        rng, carry_rng = jax.random.split(rng, 2)\n",
    "        carries = model.initialize_carries(\n",
    "            rng=carry_rng, batch_size=batch_size, hidden_sizes=[N, N]\n",
    "        )\n",
    "        # print('Apply model')\n",
    "        grads, loss, accuracy, _ = apply_model(\n",
    "            state=state,\n",
    "            carries=carries,\n",
    "            input=input,\n",
    "            target=target,\n",
    "            vocab_size=vocab_size,\n",
    "        )\n",
    "        # print('Update model')\n",
    "        state = update_model(state, grads)\n",
    "        epoch_loss.append(loss)\n",
    "        epoch_accuracy.append(accuracy)\n",
    "    train_loss = jnp.mean(jnp.array(epoch_loss))\n",
    "    train_accuracy = jnp.mean(jnp.array(epoch_accuracy))\n",
    "    return state, train_loss, train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    epochs,\n",
    "    train_data,\n",
    "    test_data,\n",
    "    block_size,\n",
    "    vocab_size,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=64,\n",
    "):\n",
    "    # print('Checkpoints')\n",
    "    # # Define the directory where checkpoints will be stored\n",
    "    # checkpoint_dir = Path(\"/tmp/my_checkpoints\")\n",
    "    # options = ocp.CheckpointManagerOptions(max_to_keep=3, save_interval_steps=2)\n",
    "    # checkpoint_manager = ocp.CheckpointManager(\n",
    "    #     checkpoint_dir, {\"model_state\": ocp.PyTreeCheckpointer()}, options=options\n",
    "    # )\n",
    "\n",
    "    # Create the dataset\n",
    "    print('Creating dataset')\n",
    "    train_x, train_y = batch_data(data=train_data, block_size=block_size)\n",
    "    test_x, test_y = batch_data(data=test_data, block_size=block_size)\n",
    "    train_x_size = len(train_x)\n",
    "    steps_per_epoch = train_x_size // batch_size\n",
    "    decay_steps = steps_per_epoch * epochs\n",
    "\n",
    "    # Intialize the scheduler\n",
    "    print('Initializing scheduler, optimizer, model state')\n",
    "    schedule = optax.warmup_cosine_decay_schedule(\n",
    "        init_value=2e-3,\n",
    "        peak_value=2e-5,\n",
    "        warmup_steps=10,\n",
    "        decay_steps=decay_steps,\n",
    "        end_value=0.0,\n",
    "    )\n",
    "\n",
    "    # Initialization of the optimizer\n",
    "    optimizer = optax.adam(learning_rate)\n",
    "\n",
    "    # Initialize the model state\n",
    "    carries = model.initialize_carries(\n",
    "        rng=subkeys[7], batch_size=batch_size, hidden_sizes=[N, N]\n",
    "    )\n",
    "\n",
    "    print('Checkpoints')\n",
    "    state = None\n",
    "    starting_epoch = 0\n",
    "    if False:  # checkpoint_dir.exists():\n",
    "        lastest_step = checkpoint_manager.latest_step()\n",
    "        if lastest_step is not None:\n",
    "            # Restore the latest checkpoint\n",
    "            restored = checkpoint_manager.restore(lastest_step)\n",
    "            if restored is not None:\n",
    "                # Unpack the checkpointed state and set the starting epoch\n",
    "                state, starting_epoch = restored[\"model_state\"], restored[\"epoch\"]\n",
    "                print(f\"Resuming training from epoch {starting_epoch}\")\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"restored is None valued despite the checkpoint directory existing\"\n",
    "                )\n",
    "\n",
    "    else:\n",
    "        # checkpoint_dir.mkdir(\n",
    "        #     parents=True, exist_ok=True\n",
    "        # )  # Ensure the checkpoint directory is created\n",
    "\n",
    "        # Initialize the model parameters\n",
    "        params = model.init(\n",
    "            subkeys[1],\n",
    "            x=jnp.ones((batch_size, block_size), dtype=jnp.int32),\n",
    "            carry=carries,\n",
    "            targets=None,\n",
    "        )\n",
    "\n",
    "        state = train_state.TrainState.create(\n",
    "            apply_fn=model.apply, params=params, tx=optimizer\n",
    "        )\n",
    "        starting_epoch = 0\n",
    "        print(\"Starting training from scratch\")\n",
    "\n",
    "    # state = None\n",
    "    # starting_epoch = 0\n",
    "\n",
    "    print(f\"Model Size: {sum(x.size for x in jax.tree_leaves(state.params))}\")\n",
    "\n",
    "    print('Epochs')\n",
    "    rng = subkeys[7]\n",
    "    for epoch in range(starting_epoch, epochs):\n",
    "        print(f'Training epoch {epoch} / {epochs}')\n",
    "\n",
    "        rng, input_rng, carry_rng, test_rng = jax.random.split(rng, 4)\n",
    "        state, loss, accuracy = train_epoch(\n",
    "            rng=input_rng,\n",
    "            model=model,\n",
    "            train_ds=(train_x, train_y),\n",
    "            state=state,\n",
    "            vocab_size=vocab_size,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "\n",
    "        print('Testing epoch {epoch} / {epochs}')\n",
    "\n",
    "        indices = jax.random.randint(\n",
    "            test_rng,\n",
    "            shape=(batch_size,),\n",
    "            minval=0,\n",
    "            maxval=(test_x.shape[0] - 1),\n",
    "        )\n",
    "        test_input = test_x[indices, ...]\n",
    "        test_target = test_y[indices, ...]\n",
    "        carries = model.initialize_carries(\n",
    "            rng=carry_rng, batch_size=batch_size, hidden_sizes=[N, N]\n",
    "        )\n",
    "        _, test_loss, test_accuracy, test_logits = apply_model(\n",
    "            state=state,\n",
    "            carries=carries,\n",
    "            input=test_input,\n",
    "            target=test_target,\n",
    "            vocab_size=vocab_size,\n",
    "        )\n",
    "        if epoch%100==0:\n",
    "            print(\n",
    "                f\"Epoch: {epoch}\\n\\tTrain Loss: {loss}\\n\\tTrain Accuracy: {accuracy}\\n\\tTest Loss: {test_loss}\\n\\tTest Accuracy: {test_accuracy}\\n\\n\"\n",
    "            )\n",
    "            print(f\"Sample Model Text Output:\")\n",
    "            print(f\"------------------------------------------------------------\")\n",
    "            print(f\"{decode(test_logits[0].tolist())}\")\n",
    "            print(f\"------------------------------------------------------------\")\n",
    "            # checkpoint_manager.save(epoch, {\"model_state\": state})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hippo_list = [\n",
    "#     {\n",
    "#         \"name\": \"LegS-LSI\",\n",
    "#         \"use\": False,\n",
    "#         \"val\": \"legs\",\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"LegS-LTI\",\n",
    "#         \"use\": True,\n",
    "#         \"val\": \"legs\",\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"LegT\",\n",
    "#         \"use\": False,\n",
    "#         \"val\": \"legt\",\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"LMU\",\n",
    "#         \"use\": False,\n",
    "#         \"val\": \"lmu\",\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"LagT\",\n",
    "#         \"use\": False,\n",
    "#         \"val\": \"lagt\",\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"FRU\",\n",
    "#         \"use\": False,\n",
    "#         \"val\": \"fru\",\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"FouT\",\n",
    "#         \"use\": False,\n",
    "#         \"val\": \"fout\",\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"FouD\",\n",
    "#         \"use\": False,\n",
    "#         \"val\": \"foud\",\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"ChebT\",\n",
    "#         \"use\": False,\n",
    "#         \"val\": \"chebt\",\n",
    "#     },\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discretization = [\n",
    "#     {\n",
    "#         \"name\": \"Forward-Euler\",\n",
    "#         \"use\": False,\n",
    "#         \"val\": 0.0,\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"Backward-Euler\",\n",
    "#         \"use\": False,\n",
    "#         \"val\": 1.0,\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"Bilinear\",\n",
    "#         \"use\": False,\n",
    "#         \"val\": 0.5,\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"Zero-Order Hold\",\n",
    "#         \"use\": True,\n",
    "#         \"val\": 2.0,\n",
    "#     },\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this network will be depth of 2\n",
    "# karpathy shows that a depth=2 LSTM works well with TinyShakespeare (depth=1 is fail)\n",
    "cell_list = [BatchedGatedHiPPOCell, BatchedGatedHiPPOCell]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discretizations:\n",
    "    FORWARD_EULER, BACKWARD_EULER, BILINEAR, ZOH = 0.0, 1.0, 0.5, 2.0\n",
    "\n",
    "print('Initializing HiPPO-RNN with: LegS-LTI and ZOH...')\n",
    "model = initialize_rnn(cell_list=cell_list, method='legs', ALPHA=Discretizations.ZOH)\n",
    "print('... Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    model=model,\n",
    "    epochs=10000,\n",
    "    train_data=train_data,\n",
    "    test_data=val_data,\n",
    "    block_size=_block_size,\n",
    "    vocab_size=vocab_size,\n",
    "    learning_rate=lr,\n",
    "    batch_size=batch_size,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for hippo in hippo_list:\n",
    "#     if hippo[\"use\"]:\n",
    "#         alpha = -1.0\n",
    "#         for a in discretization:\n",
    "#             if a[\"use\"]:\n",
    "#                 alpha = a[\"val\"]\n",
    "#         print(f\"Running HiPPO-RNN with {hippo['name']} and {alpha}\")\n",
    "#         model = initialize_rnn(cell_list=cell_list, method=hippo[\"val\"], alpha=alpha)\n",
    "#         train(\n",
    "#             model=model,\n",
    "#             epochs=epochs,\n",
    "#             train_data=train_data,\n",
    "#             test_data=val_data,\n",
    "#             block_size=_block_size,\n",
    "#             vocab_size=vocab_size,\n",
    "#             learning_rate=lr,\n",
    "#             batch_size=batch_size,\n",
    "#         )\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
