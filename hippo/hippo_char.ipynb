{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ”¸ https://github.com/BeeGass/HiPPO-Jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q --upgrade pip\n",
    "! pip install -q einops jax jaxlib flax jaxtyping typing-extensions\n",
    "! pip install -q tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "JIT = False  # Set to False to disable JIT\n",
    "\n",
    "if JIT:\n",
    "    # TODO: set JIT=True and inspect this logfile\n",
    "    # takes > 30min to generate on my macbook\n",
    "    %env XLA_FLAGS=--xla_dump_to=/tmp/why_is_this_slow.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module_path = os.path.abspath(os.path.join(\"../../../../\"))\n",
    "# print(f\"module_path: {module_path}\")\n",
    "# if module_path not in sys.path:\n",
    "#     print(f\"Adding {module_path} to sys.path\")\n",
    "#     sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"False\"\n",
    "os.environ[\"TF_FORCE_UNIFIED_MEMORY\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "from flax.linen.activation import sigmoid, tanh, relu\n",
    "import numpy as np\n",
    "# import torch\n",
    "from flax.training import train_state  # , orbax_utils\n",
    "import orbax.checkpoint as ocp\n",
    "import optax\n",
    "from jaxtyping import install_import_hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hippo_live import HiPPOCell, HiPPOLTI\n",
    "\n",
    "from cells_live import LSTMCell, BatchedGatedHiPPOCell, CharRNN\n",
    "\n",
    "from trans import initializer, legt, legs, lmu, lagt, fru, fout, foud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CpuDevice(id=0)]\n",
      "The Device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(jax.devices())\n",
    "print(f\"The Device: {jax.lib.xla_bridge.get_backend().platform}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"MPS available: {torch.backends.mps.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_printoptions(linewidth=150)\n",
    "np.set_printoptions(linewidth=150)\n",
    "jnp.set_printoptions(linewidth=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1701\n",
    "key = jax.random.PRNGKey(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_copies = 20\n",
    "subkeys = jax.random.split(key, num=num_copies)\n",
    "key = subkeys[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters For Generating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1\n",
    "freq = 10\n",
    "step = 1 / (28 * 28)  # 1e-3\n",
    "L = int(T / step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "data_size = L\n",
    "input_size = 1\n",
    "_block_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sequences = 100\n",
    "epochs = 3\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters For HiPPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset (TinyShakespeare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../datasets/shakespeare.txt\", \"r\", encoding=\"latin-1\") as f:\n",
    "    text = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get all the unique characters that occur in this text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all the unique characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocab size: 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"all the unique characters:\", \"\".join(chars))\n",
    "print(f\"vocab size: {vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a mapping from characters to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(s):\n",
    "    int_list = []\n",
    "    for c in s:\n",
    "        if c not in stoi:\n",
    "            raise ValueError(f\"character {c} not in vocabulary\")\n",
    "        else:\n",
    "            int_list.append(\n",
    "                stoi[c]\n",
    "            )  # encoder: take a string, output a list of integers\n",
    "    return int_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(l):\n",
    "    str_list = []\n",
    "    for i in l:\n",
    "        if i not in itos:\n",
    "            raise ValueError(f\"integer {i} not in the vocabulary\")\n",
    "        else:\n",
    "            str_list.append(\n",
    "                itos[i]\n",
    "            )  # decoder: take a list of integers, output a list of characters\n",
    "    return \"\".join(str_list)  # take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 46, 43, 1, 44, 53, 62, 1, 48, 59, 51, 54, 57, 1, 53, 60, 43, 56, 1, 58, 46, 43, 1, 50, 39, 64, 63, 1, 42, 53, 45, 8]\n",
      "The fox jumps over the lazy dog.\n"
     ]
    }
   ],
   "source": [
    "txt = \"The fox jumps over the lazy dog.\"\n",
    "print(encode(txt))\n",
    "print(decode(encode(txt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1115394,)\n",
      "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43 39\n",
      " 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 37\n",
      " 53 59]\n"
     ]
    }
   ],
   "source": [
    "data = jnp.array(encode(text))\n",
    "print(data.shape)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now split up the data into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 1,003,854 tokens\n",
      "train has shape (1003854,)\n",
      "\n",
      "val has 111,540 tokens\n",
      "val has shape (111540,)\n"
     ]
    }
   ],
   "source": [
    "n = int(0.9 * len(data))  # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(f\"train has {len(train_data):,} tokens\")\n",
    "print(f\"train has shape {train_data.shape}\\n\")\n",
    "print(f\"val has {len(val_data):,} tokens\")\n",
    "print(f\"val has shape {val_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([18, 47, 56, 57, 58,  1, 15, 47, 58], dtype=int32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[: block_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is [18] the target: 47\n",
      "when input is [18 47] the target: 56\n",
      "when input is [18 47 56] the target: 57\n",
      "when input is [18 47 56 57] the target: 58\n",
      "when input is [18 47 56 57 58] the target: 1\n",
      "when input is [18 47 56 57 58  1] the target: 15\n",
      "when input is [18 47 56 57 58  1 15] the target: 47\n",
      "when input is [18 47 56 57 58  1 15 47] the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1 : block_size + 1]\n",
    "for t in range(block_size):\n",
    "    context = x[: t + 1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(data, block_size=8):\n",
    "    # Trim the data to ensure that it is divisible by block_size for proper reshaping\n",
    "    num_complete_blocks = (data.size - 1) // block_size\n",
    "    trimmed_data_size = num_complete_blocks * block_size + 1\n",
    "    data = data[:trimmed_data_size]\n",
    "\n",
    "    # Prepare the input data 'x'\n",
    "    x = data[:-1].reshape(num_complete_blocks, block_size)\n",
    "\n",
    "    # Prepare the target data 'y'\n",
    "    # The target for each sequence in 'x' is the sequence shifted by one token\n",
    "    y = data[1:].reshape(num_complete_blocks, block_size)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_target_relation(x_ds, y_ds, bs, block_size):\n",
    "    data_size = len(x_ds)\n",
    "    target_size = len(y_ds)\n",
    "    steps_per_epoch = data_size // bs\n",
    "\n",
    "    perms = jax.random.permutation(subkeys[0], data_size)\n",
    "    perms = perms[: steps_per_epoch * bs]  # skip incomplete batch\n",
    "    perms = perms.reshape((steps_per_epoch, bs))\n",
    "\n",
    "    for i, perm in enumerate(perms):\n",
    "        print(f\"inputs: {i}\")\n",
    "        print(x_ds[perm, ...].shape)\n",
    "        print(f\"{x_ds[perm, ...]}\\n\")\n",
    "        print(f\"targets: {i}\")\n",
    "        print(y_ds[perm, ...].shape)\n",
    "        print(y_ds[perm, ...])\n",
    "        print(\"----\")\n",
    "        for b in range(bs):  # batch dimension\n",
    "            for t in range(block_size):  # time dimension\n",
    "                context = x_ds[perm, ...][b, : t + 1]\n",
    "                target = y_ds[perm, ...][b, t]\n",
    "                print(f\"when input is {context.tolist()} the target: {target}\")\n",
    "        print(\"--------\")\n",
    "        if i == 2:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_ds shape: (125481, 8)\n",
      "y_ds shape: (125481, 8)\n"
     ]
    }
   ],
   "source": [
    "x_ds, y_ds = batch_data(data=train_data, block_size=block_size)\n",
    "print(f\"x_ds shape: {x_ds.shape}\")\n",
    "print(f\"y_ds shape: {y_ds.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_target_relation(x_ds, y_ds, bs=4, block_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_rnn(cell_list, method: str, alpha: float = 2.0):\n",
    "    assert method in [\"legs\", \"legt\", \"lmu\", \"lagt\", \"fru\", \"fout\", \"foud\"]\n",
    "    init_fn = {\n",
    "        \"legs\": legs,\n",
    "        \"legt\": legt,\n",
    "        \"lmu\": lmu,\n",
    "        \"lagt\": lagt,\n",
    "        \"fru\": fru,\n",
    "        \"fout\": fout,\n",
    "        \"foud\": foud,\n",
    "        # \"chebt\": chebt,\n",
    "    }[method]\n",
    "\n",
    "    cell_args = [\n",
    "        {\n",
    "            \"hippo_cell\": HiPPOLTI,  # HiPPOLTICell,\n",
    "            \"hippo_args\": {\n",
    "                \"step_size\": step,\n",
    "                \"basis_size\": T,\n",
    "                \"alpha\": alpha,\n",
    "                # \"recon\": False,\n",
    "                \"A_init\": init_fn,\n",
    "                \"B_init\": init_fn,\n",
    "            },\n",
    "            \"tau_args\": {\n",
    "                \"features\": N,\n",
    "                \"bias\": True,\n",
    "                \"gate_fn\": sigmoid,\n",
    "                \"activation_fn\": tanh,\n",
    "                \"dtype\": jnp.float32,\n",
    "            },\n",
    "            # \"mlp_args\": {\n",
    "            #     \"features\": [1],\n",
    "            #     \"activation_fn\": relu,\n",
    "            #     \"bias\": False,\n",
    "            #     \"dtype\": jnp.float32,\n",
    "            # },\n",
    "            \"_tau\": LSTMCell,\n",
    "            \"bias\": True,\n",
    "            \"dtype\": jnp.float32,\n",
    "        }\n",
    "        for i in range(len(cell_list))\n",
    "    ]\n",
    "\n",
    "    rnn = CharRNN(\n",
    "        vocab_size=vocab_size,\n",
    "        hidden_size=N,\n",
    "        rnn_cells=cell_list,\n",
    "        cell_args=cell_args,\n",
    "    )\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_shaping():\n",
    "    # Define the model\n",
    "    model = initialize_rnn(\n",
    "        cell_list=[BatchedGatedHiPPOCell, BatchedGatedHiPPOCell],\n",
    "        method=\"legs\",\n",
    "        alpha=2.0,\n",
    "    )\n",
    "\n",
    "    # Define the input\n",
    "    input_data = jnp.ones((batch_size, block_size), dtype=jnp.int32)\n",
    "    print(f\"input data shape: {input_data.shape}\")\n",
    "\n",
    "    # Initialize the model carries\n",
    "    carries = model.initialize_carries(\n",
    "        rng=subkeys[7], batch_size=batch_size, hidden_sizes=[N, N]\n",
    "    )\n",
    "    print(f\"hidden shape: {carries}\")\n",
    "    print(f\"hidden shape: {carries[-1][0].shape}\")\n",
    "\n",
    "    # Initialize the model parameters\n",
    "    params = model.init(\n",
    "        subkeys[1],\n",
    "        x=input_data,\n",
    "        carry=carries,\n",
    "        targets=None,\n",
    "    )\n",
    "\n",
    "    # Initialize the model carries\n",
    "    carries = model.initialize_carries(\n",
    "        rng=subkeys[8], batch_size=batch_size, hidden_sizes=[N, N]\n",
    "    )\n",
    "    print(f\"hidden shape: {carries[-1][0].shape}\")\n",
    "\n",
    "    # Apply the model to the input\n",
    "    output, new_carries = model.apply(\n",
    "        params, x=input_data, carry=carries, targets=None\n",
    "    )\n",
    "\n",
    "    # Check the output shape\n",
    "    print(f\"output shape: {output.shape}\")\n",
    "    assert output.shape == (batch_size, block_size, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input data shape: (64, 8)\n",
      "hidden shape: [(Array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), Array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)), (Array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), Array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32))]\n",
      "hidden shape: (64, 128)\n",
      "hidden shape: (64, 128)\n",
      "output shape: (64, 8, 65)\n"
     ]
    }
   ],
   "source": [
    "test_shaping()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "The cells below will be dedicated towards training a character-level RNN on a given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model(state, carries, input, target, vocab_size, train=True):\n",
    "    def loss_fn(params):\n",
    "        # Use targets for teacher forcing if training\n",
    "        targets = target if train else None\n",
    "        logits, new_carries = state.apply_fn(\n",
    "            params, x=input, carry=carries, targets=targets\n",
    "        )\n",
    "\n",
    "        # One-hot encode the target with the correct vocabulary size\n",
    "        one_hot = jax.nn.one_hot(target, vocab_size)\n",
    "        loss = jnp.mean(optax.softmax_cross_entropy(logits=logits, labels=one_hot))\n",
    "        return loss, (logits, new_carries)\n",
    "\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, (logits, new_carries)), grads = grad_fn(state.params)\n",
    "\n",
    "    # Ensure accuracy calculation matches the target structure\n",
    "    accuracy = jnp.mean(jnp.argmax(logits, -1) == target)\n",
    "    return grads, loss, accuracy, jnp.argmax(logits, axis=-1)\n",
    "\n",
    "if JIT:\n",
    "    apply_model = partial(jax.jit, static_argnames=[\"vocab_size\"])(apply_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model(state, grad):\n",
    "    return state.apply_gradients(grads=grad)\n",
    "\n",
    "if JIT:\n",
    "    update_model = jax.jit(update_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(rng, model, train_ds, state, vocab_size, batch_size=64):\n",
    "    \"\"\"Train for a single epoch.\"\"\"\n",
    "    train_x, train_y = train_ds\n",
    "    rng, permute_rng = jax.random.split(rng, 2)\n",
    "    data_size = len(train_x)\n",
    "    steps_per_epoch = data_size // batch_size\n",
    "    perms = jax.random.permutation(subkeys[0], data_size)\n",
    "    perms = perms[: steps_per_epoch * batch_size]  # skip incomplete batch\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "    epoch_loss = []\n",
    "    epoch_accuracy = []\n",
    "    for perm in tqdm(perms):\n",
    "        input = train_x[perm, ...]\n",
    "        target = train_y[perm, ...]\n",
    "        rng, carry_rng = jax.random.split(rng, 2)\n",
    "        carries = model.initialize_carries(\n",
    "            rng=carry_rng, batch_size=batch_size, hidden_sizes=[N, N]\n",
    "        )\n",
    "        # print('Apply model')\n",
    "        grads, loss, accuracy, _ = apply_model(\n",
    "            state=state,\n",
    "            carries=carries,\n",
    "            input=input,\n",
    "            target=target,\n",
    "            vocab_size=vocab_size,\n",
    "        )\n",
    "        # print('Update model')\n",
    "        state = update_model(state, grads)\n",
    "        epoch_loss.append(loss)\n",
    "        epoch_accuracy.append(accuracy)\n",
    "    train_loss = jnp.mean(jnp.array(epoch_loss))\n",
    "    train_accuracy = jnp.mean(jnp.array(epoch_accuracy))\n",
    "    return state, train_loss, train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    epochs,\n",
    "    train_data,\n",
    "    test_data,\n",
    "    block_size,\n",
    "    vocab_size,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=64,\n",
    "):\n",
    "    print('Checkpoints')\n",
    "    # Define the directory where checkpoints will be stored\n",
    "    checkpoint_dir = Path(\"/tmp/my_checkpoints\")\n",
    "    options = ocp.CheckpointManagerOptions(max_to_keep=3, save_interval_steps=2)\n",
    "    checkpoint_manager = ocp.CheckpointManager(\n",
    "        checkpoint_dir, {\"model_state\": ocp.PyTreeCheckpointer()}, options=options\n",
    "    )\n",
    "\n",
    "    # Create the dataset\n",
    "    print('Creating dataset')\n",
    "    train_x, train_y = batch_data(data=train_data, block_size=block_size)\n",
    "    test_x, test_y = batch_data(data=test_data, block_size=block_size)\n",
    "    train_x_size = len(train_x)\n",
    "    steps_per_epoch = train_x_size // batch_size\n",
    "    decay_steps = steps_per_epoch * epochs\n",
    "\n",
    "    # Intialize the scheduler\n",
    "    print('Initializing scheduler, optimizer, model state')\n",
    "    schedule = optax.warmup_cosine_decay_schedule(\n",
    "        init_value=2e-3,\n",
    "        peak_value=2e-5,\n",
    "        warmup_steps=10,\n",
    "        decay_steps=decay_steps,\n",
    "        end_value=0.0,\n",
    "    )\n",
    "\n",
    "    # Initialization of the optimizer\n",
    "    optimizer = optax.adam(learning_rate)\n",
    "\n",
    "    # Initialize the model state\n",
    "    carries = model.initialize_carries(\n",
    "        rng=subkeys[7], batch_size=batch_size, hidden_sizes=[N, N]\n",
    "    )\n",
    "\n",
    "    print('Checkpoints')\n",
    "    state = None\n",
    "    starting_epoch = 0\n",
    "    if checkpoint_dir.exists():\n",
    "        lastest_step = checkpoint_manager.latest_step()\n",
    "        if lastest_step is not None:\n",
    "            # Restore the latest checkpoint\n",
    "            restored = checkpoint_manager.restore(lastest_step)\n",
    "            if restored is not None:\n",
    "                # Unpack the checkpointed state and set the starting epoch\n",
    "                state, starting_epoch = restored[\"model_state\"], restored[\"epoch\"]\n",
    "                print(f\"Resuming training from epoch {starting_epoch}\")\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"restored is None valued despite the checkpoint directory existing\"\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            checkpoint_dir.mkdir(\n",
    "                parents=True, exist_ok=True\n",
    "            )  # Ensure the checkpoint directory is created\n",
    "\n",
    "            # Initialize the model parameters\n",
    "            params = model.init(\n",
    "                subkeys[1],\n",
    "                x=jnp.ones((batch_size, block_size), dtype=jnp.int32),\n",
    "                carry=carries,\n",
    "                targets=None,\n",
    "            )\n",
    "\n",
    "            state = train_state.TrainState.create(\n",
    "                apply_fn=model.apply, params=params, tx=optimizer\n",
    "            )\n",
    "            starting_epoch = 0\n",
    "            print(\"Starting training from scratch\")\n",
    "\n",
    "    print('Epochs')\n",
    "    rng = subkeys[7]\n",
    "    for epoch in range(starting_epoch, epochs):\n",
    "        print(f'Training epoch {epoch} / {epochs}')\n",
    "\n",
    "        rng, input_rng, carry_rng, test_rng = jax.random.split(rng, 4)\n",
    "        state, loss, accuracy = train_epoch(\n",
    "            rng=input_rng,\n",
    "            model=model,\n",
    "            train_ds=(train_x, train_y),\n",
    "            state=state,\n",
    "            vocab_size=vocab_size,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "\n",
    "        print('Testing epoch {epoch} / {epochs}')\n",
    "\n",
    "        indices = jax.random.randint(\n",
    "            test_rng,\n",
    "            shape=(batch_size,),\n",
    "            minval=0,\n",
    "            maxval=(test_x.shape[0] - 1),\n",
    "        )\n",
    "        test_input = test_x[indices, ...]\n",
    "        test_target = test_y[indices, ...]\n",
    "        carries = model.initialize_carries(\n",
    "            rng=carry_rng, batch_size=batch_size, hidden_sizes=[N, N]\n",
    "        )\n",
    "        _, test_loss, test_accuracy, test_logits = apply_model(\n",
    "            state=state,\n",
    "            carries=carries,\n",
    "            input=test_input,\n",
    "            target=test_target,\n",
    "            vocab_size=vocab_size,\n",
    "        )\n",
    "        print(\n",
    "            f\"Epoch: {epoch}\\n\\tTrain Loss: {loss}\\n\\tTrain Accuracy: {accuracy}\\n\\tTest Loss: {test_loss}\\n\\tTest Accuracy: {test_accuracy}\\n\\n\"\n",
    "        )\n",
    "        print(f\"Sample Model Text Output:\")\n",
    "        print(f\"------------------------------------------------------------\")\n",
    "        print(f\"{decode(test_logits[0].tolist())}\")\n",
    "        print(f\"------------------------------------------------------------\")\n",
    "        checkpoint_manager.save(epoch, {\"model_state\": state})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hippo_list = [\n",
    "#     {\n",
    "#         \"name\": \"LegS-LSI\",\n",
    "#         \"use\": False,\n",
    "#         \"val\": \"legs\",\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"LegS-LTI\",\n",
    "#         \"use\": True,\n",
    "#         \"val\": \"legs\",\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"LegT\",\n",
    "#         \"use\": False,\n",
    "#         \"val\": \"legt\",\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"LMU\",\n",
    "#         \"use\": False,\n",
    "#         \"val\": \"lmu\",\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"LagT\",\n",
    "#         \"use\": False,\n",
    "#         \"val\": \"lagt\",\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"FRU\",\n",
    "#         \"use\": False,\n",
    "#         \"val\": \"fru\",\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"FouT\",\n",
    "#         \"use\": False,\n",
    "#         \"val\": \"fout\",\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"FouD\",\n",
    "#         \"use\": False,\n",
    "#         \"val\": \"foud\",\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"ChebT\",\n",
    "#         \"use\": False,\n",
    "#         \"val\": \"chebt\",\n",
    "#     },\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discretization = [\n",
    "#     {\n",
    "#         \"name\": \"Forward-Euler\",\n",
    "#         \"use\": False,\n",
    "#         \"val\": 0.0,\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"Backward-Euler\",\n",
    "#         \"use\": False,\n",
    "#         \"val\": 1.0,\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"Bilinear\",\n",
    "#         \"use\": False,\n",
    "#         \"val\": 0.5,\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"Zero-Order Hold\",\n",
    "#         \"use\": True,\n",
    "#         \"val\": 2.0,\n",
    "#     },\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_list = [BatchedGatedHiPPOCell, BatchedGatedHiPPOCell]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing HiPPO-RNN with: LegS-LTI and ZOH...\n",
      "... Done!\n"
     ]
    }
   ],
   "source": [
    "class Discretizations:\n",
    "    FORWARD_EULER, BACKWARD_EULER, BILINEAR, ZOH = 0.0, 1.0, 0.5, 2.0\n",
    "\n",
    "print('Initializing HiPPO-RNN with: LegS-LTI and ZOH...')\n",
    "model = initialize_rnn(cell_list=cell_list, method='legs', alpha=Discretizations.ZOH)\n",
    "print('... Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoints\n",
      "Creating dataset\n",
      "Initializing scheduler, optimizer, model state\n",
      "Checkpoints\n",
      "Starting training from scratch\n",
      "Epochs\n",
      "Training epoch 0 / 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [48:32<00:00, 97.09s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing epoch {epoch} / {epochs}\n",
      "Epoch: 0\n",
      "\tTrain Loss: 3.9151813983917236\n",
      "\tTrain Accuracy: 0.15810446441173553\n",
      "\tTest Loss: 3.3927512168884277\n",
      "\tTest Accuracy: 0.149566650390625\n",
      "\n",
      "\n",
      "Sample Model Text Output:\n",
      "------------------------------------------------------------\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
      "------------------------------------------------------------\n",
      "Training epoch 1 / 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [47:36<00:00, 95.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing epoch {epoch} / {epochs}\n",
      "Epoch: 1\n",
      "\tTrain Loss: 3.1012654304504395\n",
      "\tTrain Accuracy: 0.20543111860752106\n",
      "\tTest Loss: 2.8244597911834717\n",
      "\tTest Accuracy: 0.257843017578125\n",
      "\n",
      "\n",
      "Sample Model Text Output:\n",
      "------------------------------------------------------------\n",
      " te e e  t et tee ee te   t  e ee te et eee  te t t  te   t   te eee e   ee  t e eeee  eee e ee ee   t  e eeeeeeei iiiix ti  te te t et e  t e eeee eeeeeiix ii t  e t  t    eeeeee tiixxx t  e t  t    eeeee   te t  e t  e ee te e  te e t  t    eee t t e t  te e  t  e ee ee   e  ee eeeeeeiix ii te t    e  t e eeeeee iiiix ti  te teeeee tiixxx te tet ee teee t  eeeeeee iiiix t    t e   teee eeeeeixx ii t  et ee   eeee e ee   e   e te  t    eeet e e t  e t  eeeeeeeeiiixxx ti t eeeeee ee e te  e te   te t    te \n",
      "------------------------------------------------------------\n",
      "Training epoch 2 / 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [47:07<00:00, 94.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing epoch {epoch} / {epochs}\n",
      "Epoch: 2\n",
      "\tTrain Loss: 2.4194655418395996\n",
      "\tTrain Accuracy: 0.36268311738967896\n",
      "\tTest Loss: 1.9905269145965576\n",
      "\tTest Accuracy: 0.556060791015625\n",
      "\n",
      "\n",
      "Sample Model Text Output:\n",
      "------------------------------------------------------------\n",
      " she a ordinin enini   where too  aaeni we es weet toaethers hen  o wons oe the theni that weens theer w  oen horih  itt e were a oos a eet weth  itt e wen iniet eit ene a sth we   w oo ort were and a  anio i to her and so she wee  s to heinio  i an  orih and woo not  iee a wasee\n",
      "\n",
      "iii ii ie\n",
      " e   wanst thor wooi and haeen we tha saeenini t we thor aroi  wor sone  nhaeen wordsin\n",
      "ii iiiiiieni i to the a ooai as worntaens are wor wen sen het shaie note thorah then w oo aeriet a  oin\n",
      "iii ii ie\n",
      "ior  ooi wo w ien \n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model=model,\n",
    "    epochs=epochs,\n",
    "    train_data=train_data,\n",
    "    test_data=val_data,\n",
    "    block_size=_block_size,\n",
    "    vocab_size=vocab_size,\n",
    "    learning_rate=lr,\n",
    "    batch_size=batch_size,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for hippo in hippo_list:\n",
    "#     if hippo[\"use\"]:\n",
    "#         alpha = -1.0\n",
    "#         for a in discretization:\n",
    "#             if a[\"use\"]:\n",
    "#                 alpha = a[\"val\"]\n",
    "#         print(f\"Running HiPPO-RNN with {hippo['name']} and {alpha}\")\n",
    "#         model = initialize_rnn(cell_list=cell_list, method=hippo[\"val\"], alpha=alpha)\n",
    "#         train(\n",
    "#             model=model,\n",
    "#             epochs=epochs,\n",
    "#             train_data=train_data,\n",
    "#             test_data=val_data,\n",
    "#             block_size=_block_size,\n",
    "#             vocab_size=vocab_size,\n",
    "#             learning_rate=lr,\n",
    "#             batch_size=batch_size,\n",
    "#         )\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
